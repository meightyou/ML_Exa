# ------------ MACHINE LEARNING CODE ------------------

```{r, echo = FALSE, message = FALSE, warning = FALSE}
source(here::here("scripts/setup.R"))
```

# Data Loading

```{r, echo = T, message = FALSE,warning=FALSE}
DocVis <- read_csv("DocVis.csv")

german <- read_csv("german.csv")

nursing_data <- read_csv("nursing_data.csv")

real_estate <- read_csv("real_estate_data.csv")

Wine <- read_csv("Wine.csv")

imports.85 <- read.csv("imports-85.data", 
                       header=FALSE, na.strings="?")

```

# Course 0

## EDA

### Data Rename

```{r, echo = T, message = FALSE,warning=FALSE}
names(imports.85) <- c("symboling", "normalized_losses", "make", "fuel_type", "aspiration",
                       "num_doors", "body_style", "drive_wheels", "engine_location", "wheel_base",
                       "length", "width", "height","curb_weight", "engine_type", "num_cylinders",
                       "engine_size", "fuel_system", "bore", "stroke", "compression_ratio", "horsepower",
                       "peak_rpm", "city_mpg", "highway_mpg", "price")
```

### Data structure and summary

```{r, echo = T, message = FALSE,warning=FALSE}

str(imports.85)
head(imports.85)
summary(imports.85)

# Turning character columns into factors through a loop
for (i in 1:ncol(imports.85)){
  if (class(imports.85[,i])=="character"){
    imports.85[,i] <- factor(imports.85[,i])
  }
}
# same operation can also be done through the `lapply` function
# imports.85[sapply(imports.85, is.character)] <-
#   lapply(imports.85[sapply(imports.85, is.character)],
#          as.factor)

str(imports.85)
summary(imports.85)

dfSummary(imports.85, style="grid")

view(dfSummary(imports.85, style="grid",plain.ascii = FALSE, tmp.img.dir = "/tmp"))

## first look at the current levels
levels(imports.85$fuel_system)
## Now rename the levels merging 4bbl[3], mfi[5], spfi[8] to "other"
levels(imports.85$fuel_system)[c(3,5,8)] <- "other"

## see the result
summary(imports.85)

```

### Graph for each variable

```{r, echo = T, message = FALSE,warning=FALSE}

## Individually
p1 <- ggplot(imports.85, aes(x="price", y=price)) + 
  geom_boxplot()+xlab("")
p1
p2 <- ggplot(imports.85, aes(x="price", y=price)) + 
  geom_violin()+xlab("")
p2
p3 <- ggplot(imports.85, aes(x=price)) + 
  geom_histogram(color="black", fill="white")
p3
## The 3 plots on the same page
grid.arrange(p1, p2, p3, nrow=2, layout_matrix=matrix(c(1,2,3,3), byrow=TRUE, nc=2))

## pie chart with the basic plot function
## (ggplot2 is not good at producing pie charts)
pie(table(imports.85$fuel_system))

## bar plots
imports.85 %>% count(fuel_system) %>%
  ggplot(aes(y=n, x=fuel_system)) + 
  geom_bar(stat="identity")+coord_flip()

imports.85 %>% count(fuel_system) %>%
  ggplot(aes(fill=fuel_system, y=n, x="Fuel System")) + 
  geom_bar(position="stack", stat="identity", color="black")+xlab("")


## pie chart with the basic plot function
## (ggplot2 is not good at producing pie charts)
pie(table(imports.85$fuel_system))

## bar plots
imports.85 %>% count(fuel_system) %>%
  ggplot(aes(y=n, x=fuel_system)) + 
  geom_bar(stat="identity")+coord_flip()

imports.85 %>% count(fuel_system) %>%
  ggplot(aes(fill=fuel_system, y=n, x="Fuel System")) + 
  geom_bar(position="stack", stat="identity", color="black")+xlab("")
```

### Several graphs

```{r, echo = T, message = FALSE,warning=FALSE}

## For the numerical variables
imports.85 %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()

## For the categorical variables
imports.85 %>%
  keep(is.factor) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_bar(stat="count")+coord_flip()

imports.85 %>%
  keep(is.numeric) %>% 
  gather()
```

### Scatterplot matrix for several variables

```{r, echo = T, message = FALSE,warning=FALSE}
library(GGally)

imports.85 %>% select(length, height, width, price) %>% 
 ggpairs()

imports.85 %>% select(length, height, width, price, fuel_system) %>% 
  ggpairs(columns=1:4, aes(colour=fuel_system))

imports.85 %>% select(fuel_system, fuel_type, aspiration) %>% 
  ggpairs()

imports.85 %>% select(body_style, num_doors, width, price) %>% 
  ggpairs()

```

## Recoding and feature engineering

### Building a new feature

```{r, echo = T, message = FALSE,warning=FALSE}

imports.85 %>% mutate(volume=height*width*length) %>% 
  select(price, height, width, length, volume) %>% ggpairs
```

### Recoding a variable

```{r, echo = T, message = FALSE,warning=FALSE}

imports.85 %>% mutate(num_cyl_num=case_when(
  num_cylinders=="two"~2, 
  num_cylinders=="three"~3, 
  num_cylinders=="four"~4, 
  num_cylinders=="five"~5, 
  num_cylinders=="six"~6, 
  num_cylinders=="eight"~8, 
  num_cylinders=="twelve"~12)) %>% 
  select(price, num_cyl_num, num_cylinders) %>% ggpairs

library(fastDummies)
imports.85 %>% select(fuel_system) %>% dummy_cols(remove_first_dummy = TRUE)

```



# COURSE 1 : K-Nearest Neighbours

## 1 K-NN: a gentle introduction

```{r, echo = T, message = FALSE,warning=FALSE}
library(dplyr)
library(caret)
iris
iris <- iris

mod.knn <- knn3(data = iris, Species~., k = 2) ## build the knn model
predict(mod.knn, newdata = iris, type = "class")

set.seed(123) ## for replication purpose

index.tr <- sample(1:nrow(iris), replace=FALSE, size=0.75*nrow(iris))
index.tr ## the index of the rows of iris that will be in the training set

iris.tr <- iris[index.tr,] ## the training set
iris.te <- iris[-index.tr,] ## the test set, the minus sign is telling take ALL BUT the training set

mod.knn <- knn3(data=iris.tr, Species~., k=2)
iris.te.pred <- predict(mod.knn, newdata = iris.te, type="class")

table(Pred=iris.te.pred, Observed=iris.te[,5])

mod.knn <- knn3(data=iris.tr, Species~., k=3)
iris.te.pred <- predict(mod.knn, newdata = iris.te, type="class")

table(Pred=iris.te.pred, Observed=iris.te[,5])

class(mod.knn) # class = knn

```

## 2 K-NN REGRESSION

```{r, echo = T, message = FALSE,warning=FALSE}

tmp <- imports.85 %>% select(where(is.numeric))#only select columns with numerical data
tmp <- filter(tmp, complete.cases(tmp))#and then only select rows where there is no NAs

tmp# tmp means temporary

set.seed(123)
index.tr <- sample(1:nrow(tmp), size=0.75*nrow(tmp), replace = FALSE)
tmp.tr <- tmp[index.tr,]
tmp.te <- tmp[-index.tr,]
mod.knn <- knnreg(data=tmp.tr, price~., k=3)# 3 nearest neighbours
tmp.pred <- predict(mod.knn, newdata=tmp.te)

tmp.tr#just to view
tmp.te#just to view

tmp.te %>% mutate(pred=tmp.pred) %>%
  ggplot(aes(x=price, y=pred)) + 
  geom_point() + geom_abline(slope=1, intercept = 0)

```

## 3 K-NN: mixture of feature types

```{r, echo = T, message = FALSE,warning=FALSE}

## tmp is imports.85 without incomplete cases and without price
imp.comp <- imports.85 %>% filter(complete.cases(imports.85))#select only rows with no missing data
y <- imp.comp$price#y variable
imp.comp <- imp.comp %>% select(!price)#x variable = all but y variable

## The numerical variables in tmp are standardized
imp.num <- imp.comp %>% select(where(is.numeric)) %>% scale() %>% as.data.frame()

## the dummy variables of the numerical features of tmp are created
library(fastDummies)
imp.dumm <- imp.comp %>% select(!where(is.numeric)) %>% 
  dummy_cols(remove_first_dummy = FALSE, remove_selected_columns = TRUE)

## These dummy variables are added to tmp
imp.dat <- data.frame(imp.num, imp.dumm)

## The names of the two problematic brands are changed
names(imp.dat)[c(16,25)] <- c("make_alfa_romeo","make_mercredes_benz")

## The raw price is added to tmp
imp.dat$price <- y

set.seed(123)
index.tr <- sample(1:nrow(imp.dat), size=0.75*nrow(imp.dat), replace = FALSE)
imp.tr <- imp.dat[index.tr,]
imp.te <- imp.dat[-index.tr,]
mod.knn <- knnreg(data=imp.tr, price~., k=3)
imp.pred <- predict(mod.knn, newdata=imp.te)
imp.te %>% mutate(pred=imp.pred) %>%
  ggplot(aes(x=price, y=pred)) + 
  geom_point() + geom_abline(slope=1, intercept = 0)

```

# Course 2 Models: Naive Bayes

## 1 Conditional density plots

```{r, echo = T, message = FALSE,warning=FALSE}

library(naivebayes)
iris.nb <- naive_bayes(Species ~ .,
                       data = iris, usekernel=TRUE)
par(mfrow=c(2,2))
plot(iris.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8),
     prob="conditional")
par(mfrow=c(1,1))

iris.nb

iris.nb <- naive_bayes(Species ~ .,
                       data = iris, usekernel=FALSE) 
par(mfrow=c(2,2))
plot(iris.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8), prob="conditional")
par(mfrow=c(1,1))

iris.nb

```

## 2 Predictions

```{r, echo = T, message = FALSE,warning=FALSE}

predict(iris.nb, newdata = iris[,-5])
predict(iris.nb, newdata = iris[,-5], type="prob")

```

## 3 Priors

```{r, echo = T, message = FALSE,warning=FALSE}

iris.nb <- naive_bayes(Species ~ .,
                       data = iris, usekernel=FALSE, prior = c(0.1,0.9,0.1)) 
par(mfrow=c(2,2))
plot(iris.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8), prob="conditional")
par(mfrow=c(1,1))

predict(iris.nb, newdata = iris[,-5])

```

## 4 Categorical features

```{r, echo = T, message = FALSE,warning=FALSE}

DocVis.nb <- naive_bayes(visits~., data=DocVis, usekernel = TRUE)
plot(DocVis.nb, prob = "conditional")

table(pred=predict(DocVis.nb), obs=DocVis$visits)

```


# Course 3 Models: Linear and logistic regressions

## 1 Linear regression for real estate transaction price predictions

### 1.1 The data

```{r, echo = T, message = FALSE,warning=FALSE}

real_estate <- read_csv("real_estate_data.csv")

```

### 1.2 EDA

```{r, echo = T, message = FALSE,warning=FALSE}

real_estate_data <- read.csv("real_estate_data.csv") ## adapt the path to the data
# if you encountered any error with the encoding of the data (`Error in gregexpr...`), just re-run the code again
str(real_estate_data)
library(summarytools)
dfSummary(real_estate_data)

library(dplyr)
library(ggplot2)
real_estate_data %>% ggplot(aes(x=Month, y=Price, fill=as.factor(Year))) + 
  geom_boxplot()+ facet_wrap(~as.factor(Year))

library(GGally)
real_estate_data %>% 
  select(Price, HouseAge, Dist, Lat, Long, TransDate) %>% 
  ggpairs()

```

### 1.3 Linear regression

```{r, echo = T, message = FALSE,warning=FALSE}

set.seed(234)
index <- sample(x=c(1,2), size=nrow(real_estate_data), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set
dat.tr <- real_estate_data[index==1,]
dat.te <- real_estate_data[index==2,]

mod.lm <- lm(Price~TransDate+
               HouseAge+
               Dist+
               NumStores+
               Lat+
               Long, data=dat.tr)
summary(mod.lm)

```

### 1.4 Variable selection and interpretation

```{r, echo = T, message = FALSE,warning=FALSE}

step(mod.lm) # see the result
mod.lm.sel <- step(mod.lm) # store the final model into mod.lm.sel
summary(mod.lm.sel)

```

### 1.5 Predictions

```{r, echo = T, message = FALSE,warning=FALSE}

mod.lm.pred <- predict(mod.lm.sel, newdata=dat.te)
plot(dat.te$Price ~ mod.lm.pred, xlab="Prediction", ylab="Observed prices")
abline(0,1) # line showing the obs -- pred agreement

#Answer
mod.lm.pred[c(1,2)]
```

## 2 Logistic regression with visit data

### 2.1 The data

```{r, echo = T, message = FALSE,warning=FALSE}

DocVis <- read.csv("DocVis.csv") ## adapt the path
DocVis$visits <- ifelse(DocVis$visits=="Yes",1,0)

```

### 2.2 The model

```{r, echo = T, message = FALSE,warning=FALSE}
set.seed(234)
index <- sample(x=c(1,2), size=nrow(DocVis), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set
dat.tr <- DocVis[index==1,]
dat.te <- DocVis[index==2,]

vis.logr <- glm(visits~., data=dat.tr, family="binomial")
summary(vis.logr)


```

### 2.3 Variable selection and interpretation of the coefficients

```{r, echo = T, message = FALSE,warning=FALSE}

vis.logr.sel <- step(vis.logr)
summary(vis.logr.sel)

```

### 2.4 Predictions

```{r, echo = T, message = FALSE,warning=FALSE}

prob.te <- predict(vis.logr.sel, newdata = dat.te, type="response")
pred.te <- ifelse(prob.te >= 0.5, 1, 0)
table(Pred=pred.te, Obs=dat.te$visits)

boxplot(prob.te~dat.te$visits)

#Answer
prob.te[c(1,2)]


```
# Course 4 

## 1 Classification tree

### 1.1 Prepare the data
```{r, echo = T, message = FALSE,warning=FALSE}
library(ISLR)
library(dplyr)
MyCarseats <- Carseats %>% mutate(SaleHigh=ifelse(Sales > 7.5, "Yes", "No"))
MyCarseats <- MyCarseats %>% select(-Sales)

set.seed(123) # for reproducibility 
index.tr <- sample(x=1:nrow(MyCarseats), size=2/3*nrow(MyCarseats), replace=FALSE)
df.tr <- MyCarseats[index.tr,]
df.te <- MyCarseats[-index.tr,]

```

### 1.2 Classification tree fit and plot

```{r, echo = T, message = FALSE,warning=FALSE}
library(rpart)
library(rpart.plot)
set.seed(1234)
carseats.tree <- rpart(SaleHigh ~ ., data=df.tr)
rpart.plot(carseats.tree)

```

### 1.3 Pruning the tree

```{r, echo = T, message = FALSE,warning=FALSE}
plotcp(carseats.tree)
carseats.tree.prune <- prune(carseats.tree, cp=0.025)
rpart.plot(carseats.tree.prune)
library(adabag)
set.seed(123455)
rpart.plot(autoprune(SaleHigh ~ ., data=df.tr))

```

### 1.4 Predictions

```{r, echo = T, message = FALSE,warning=FALSE}
#answer 
MyCarseats[1,]

pred <- predict(carseats.tree.prune, newdata=df.te, type="class")
table(Pred=pred, Obs=df.te$SaleHigh)

predict(carseats.tree.prune, newdata=df.te, type="prob")

```

## 2 Regression tree

```{r, echo = T, message = FALSE,warning=FALSE}


set.seed(123)
carseats.reg <- rpart(Sales ~ ., data=Carseats)
rpart.plot(carseats.reg)

carseat.reg.pred <- predict(carseats.reg)
plot(carseat.reg.pred ~ Carseats$Sales,
     xlab="Sales", ylab="Predictions")
abline(0,1, col="red")

```


# Course 5 Models: Neural Network

## 1 Neural Network (NN)

### 1.1 Build simple NNs with nnet

```{r, echo = T, message = FALSE,warning=FALSE}

library(nnet)

nn1 <- nnet(Species~Sepal.Length+Petal.Width, data=iris, size=2)
summary(nn1)
nn2 <- nnet(Species~Sepal.Length+Petal.Width, data=iris, size=c(5,3))
nn3 <- nnet(Species~Sepal.Length+Petal.Width, data=iris, size=2, decay=3)

pred1 <- predict(nn1, type="class")
pred2 <- predict(nn2, type="class")
pred3 <- predict(nn3, type="class")

tab1 <- table(Obs=iris$Species, Pred=pred1)
tab2 <- table(Obs=iris$Species, Pred=pred2)
tab3 <- table(Obs=iris$Species, Pred=pred3)

(acc1 <- sum(diag(tab1))/sum(tab1))
(acc2 <- sum(diag(tab2))/sum(tab2))
(acc3 <- sum(diag(tab3))/sum(tab3))

```

### 1.2 nnet+caret: Simple hyperparameter tuning

```{r, echo = T, message = FALSE,warning=FALSE}

library(caret)
# this can take a few seconds to run on your computer
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           #`cv` stands for `cross-validation` which we will 
                           #see during one of the upcoming courses
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1),
                         decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(Species ~ Sepal.Length+Petal.Width, 
                 data = iris,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

plot(nnetFit)

```

### 1.2.1 Visualizing NNs

```{r, echo = T, message = FALSE,warning=FALSE}

library(neuralnet)
Species.hot <- class.ind(iris$Species) 
iris.hot <- cbind(iris[,1:4], Species.hot)
head(iris.hot)

f <- as.formula(paste("setosa+versicolor+virginica~", paste(names(iris.hot)[1:4], collapse = " + ")))
f
nn4 <- neuralnet(f, data=iris.hot, hidden=2)
nn5 <- neuralnet(f, data=iris.hot, hidden=c(2,3))
plot(nn4, rep="best")
plot(nn5, rep="best")

```

# Couse 6 Models: Support Vector Machine

## 1 Support Vector Machine (SVM)

### 1.1 Prepare the data

```{r, echo = T, message = FALSE,warning=FALSE}

library(ISLR)
library(dplyr)
MyCarseats <- Carseats %>% mutate(SaleHigh=ifelse(Sales > 7.5, "Yes", "No"))
MyCarseats <- MyCarseats %>% select(-Sales)
MyCarseats$SaleHigh <- as.factor(MyCarseats$SaleHigh)

set.seed(123) # for reproducibility 
index.tr <- sample(x=1:nrow(MyCarseats), size=0.8*nrow(MyCarseats), replace=FALSE)
df.tr <- MyCarseats[index.tr,]
df.te <- MyCarseats[-index.tr,]

```

### 1.2 Linear SVM and the accuracy

```{r, echo = T, message = FALSE,warning=FALSE}

library(e1071)
carseats.svm <- svm(SaleHigh ~ ., data=df.tr, kernel="linear")
carseats.svm
carseats.svm.pred <- predict(carseats.svm, newdata = df.te)

table(Pred=carseats.svm.pred, obs=df.te$SaleHigh)

library(caret)
confusionMatrix(data=carseats.svm.pred, reference = df.te$SaleHigh )

```

### 1.3 Radial basis SVM

```{r, echo = T, message = FALSE,warning=FALSE}

carseats.rb <- svm(SaleHigh ~ ., data=df.tr, kernel="radial")
carseats.rb
carseats.rb.pred <- predict(carseats.rb, newdata = df.te)
confusionMatrix(data=carseats.rb.pred, reference = df.te$SaleHigh )

```

### 1.4 Tuning the hyperparameters

### 1.4.1 Linear SVM

```{r, echo = T, message = FALSE,warning=FALSE}

trctrl <- trainControl(method = "cv", number=10)
set.seed(143)
svm_Linear <- train(SaleHigh ~., data = df.tr, method = "svmLinear",
                    trControl=trctrl)
svm_Linear

grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid
set.seed(143)
svm_Linear_Grid <- train(SaleHigh ~., data = df.tr, method = "svmLinear",
                         trControl=trctrl,
                         tuneGrid = grid)
svm_Linear_Grid
plot(svm_Linear_Grid)
svm_Linear_Grid$bestTune

```

### 1.4.2 Radial Basis SVM

```{r, echo = T, message = FALSE,warning=FALSE}

grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial
set.seed(143)
svm_Radial_Grid <- train(SaleHigh ~., data = df.tr, method = "svmRadial",
                         trControl=trctrl,
                         tuneGrid = grid_radial)
svm_Radial_Grid
plot(svm_Radial_Grid)
svm_Radial_Grid$bestTune

```

### 1.5 Best model

```{r, echo = T, message = FALSE,warning=FALSE}
carseats.rb.tuned <- svm(SaleHigh ~ .,data = df.tr,
                         kernel = "radial", gamma = svm_Radial_Grid$bestTune$sigma,
                         cost = svm_Radial_Grid$bestTune$C)
carseats.rb.tuned.pred <- predict(carseats.rb.tuned, newdata = df.te)
confusionMatrix(data=carseats.rb.tuned.pred, reference = df.te$SaleHigh)

```

# Course 7 Model scoring

## 1 Regression task

### 1.1 Data

```{r, echo = T, message = FALSE,warning=FALSE}

library(readr)
real_estate_data <- read_csv("real_estate_data.csv") # adapt the path

library(caret)
set.seed(234)
index.tr <- createDataPartition(y = real_estate_data$Price, p= 0.8, list = FALSE)
df.tr <- real_estate_data[index.tr,]
df.te <- real_estate_data[-index.tr,]
```

### 1.2 Models

```{r, echo = T, message = FALSE,warning=FALSE}

library(rpart)
est.lm <- lm(Price~TransDate+HouseAge+Dist+
               NumStores+Lat+Long, data=df.tr)
est.rt <- rpart(Price~TransDate+HouseAge+Dist+
                  NumStores+Lat+Long, data=df.tr)
est.knn <- knnreg(Price~TransDate+HouseAge+Dist+
                    NumStores+Lat+Long, data=df.tr, k = 3)

```

### 1.3 R square

```{r, echo = T, message = FALSE,warning=FALSE}

R2(predict(est.lm, newdata = df.te), df.te$Price)
R2(predict(est.rt, newdata = df.te), df.te$Price)
R2(predict(est.knn, newdata = df.te), df.te$Price)

cor(predict(est.lm, newdata = df.te), df.te$Price)^2

```

### 1.4 RMSE

```{r, echo = T, message = FALSE,warning=FALSE}

RMSE(predict(est.lm, newdata = df.te), df.te$Price)
RMSE(predict(est.rt, newdata = df.te), df.te$Price)
RMSE(predict(est.knn, newdata = df.te), df.te$Price)

```

### 1.5 MAE

```{r, echo = T, message = FALSE,warning=FALSE}

MAE(predict(est.lm, newdata = df.te), df.te$Price)
MAE(predict(est.rt, newdata = df.te), df.te$Price)
MAE(predict(est.knn, newdata = df.te), df.te$Price)

mean(abs(predict(est.lm, newdata = df.te)-df.te$Price))

```

### 1.6 The best model

```{r, echo = T, message = FALSE,warning=FALSE}

par(mfrow=c(2,2))
plot(df.te$Price ~ predict(est.lm, newdata = df.te), xlab="Prediction", 
     ylab="Observed prices", main="Lin. Reg.")
abline(0,1)
plot(df.te$Price ~ predict(est.rt, newdata = df.te), xlab="Prediction", 
     ylab="Observed prices", main="Lin. Reg.")
abline(0,1)
plot(df.te$Price ~ predict(est.knn, newdata = df.te), xlab="Prediction", 
     ylab="Observed prices", main="Lin. Reg.")
abline(0,1)
par(mfrow=c(1,1))

```

## 2 Classification task

### 2.1 Data

```{r, echo = T, message = FALSE,warning=FALSE}
DocVis <- read.csv("DocVis.csv") ## adapt the path
DocVis$visits <- as.factor(DocVis$visits)

library(caret)
set.seed(346)
index.tr <- createDataPartition(y = DocVis$visits, p= 0.8, list = FALSE)
df.tr <- DocVis[index.tr,]
df.te <- DocVis[-index.tr,]

```

### 2.2 Models

```{r, echo = T, message = FALSE,warning=FALSE}

library(naivebayes)
library(e1071)
library(adabag)

## Logistic regression
Doc.lr <- glm(visits~., data=df.tr, family="binomial")
Doc.lr <- step(Doc.lr)

## Naive Bayes 
Doc.nb <- naive_bayes(visits~., data=df.tr, usekernel=TRUE) 

## Classification tree 
Doc.ct <- autoprune(visits~., data=df.tr)

## SVM radial basis
# grid_radial <- expand.grid(sigma = c(0.0001, 0.001, 0.01, 0.1),
#                           C = c(0.1, 1, 10, 100, 1000))
# trctrl <- trainControl(method = "cv", number=10)
# set.seed(143)
# Doc.svm <- train(visits ~., data = df.tr, method = "svmRadial",
#                          trControl=trctrl,
#                          tuneGrid = grid_radial)
Doc.svm <- svm(visits~., data=df.tr, gamma=0.001, cost=1000, probability=TRUE)
```

### 2.3 Predictions

```{r, echo = T, message = FALSE,warning=FALSE}
## Logistic regression
Doc.lr.prob <- predict(Doc.lr, newdata=df.te, type="response")
Doc.lr.pred <- ifelse(Doc.lr.prob>0.5,"Yes","No")

## Naive Bayes 
Doc.nb.prob <- predict(Doc.nb, newdata=df.te, type="prob") 
Doc.nb.pred <- predict(Doc.nb, newdata=df.te) 

## Classification tree 
Doc.ct.prob <- predict(Doc.ct, newdata=df.te, type="prob")
Doc.ct.pred <- predict(Doc.ct, newdata=df.te, type="class")

## SVM radial basis
library(dplyr)
Doc.svm.prob <- predict(Doc.svm, newdata=df.te, probability=TRUE) %>% attr("probabilities")
Doc.svm.pred <- predict(Doc.svm, newdata=df.te, type="class")

```

### 2.4 Confusion matrices and prediction-based measures

```{r, echo = T, message = FALSE,warning=FALSE}

confusionMatrix(data=as.factor(Doc.lr.pred), reference = df.te$visits)
confusionMatrix(data=as.factor(Doc.nb.pred), reference = df.te$visits)
confusionMatrix(data=as.factor(Doc.ct.pred), reference = df.te$visits)
confusionMatrix(data=as.factor(Doc.svm.pred), reference = df.te$visits)

```

### 2.5 Probability-based measures

```{r, echo = T, message = FALSE,warning=FALSE}

df.pred.lr <- data.frame(obs=df.te$visits,
                         Yes=Doc.lr.prob,
                         No=1-Doc.lr.prob,
                         pred=as.factor(Doc.lr.pred))
head(df.pred.lr)
df.pred.nb <- data.frame(obs=df.te$visits,
                         Doc.nb.prob,
                         pred=as.factor(Doc.nb.pred))
head(df.pred.nb)
df.pred.ct <- data.frame(obs=df.te$visits,
                         Doc.ct.prob,
                         pred=as.factor(Doc.ct.pred))
head(df.pred.ct)
df.pred.svm <- data.frame(obs=df.te$visits,
                          Doc.svm.prob,
                          pred=as.factor(Doc.svm.pred))
head(df.pred.svm)

twoClassSummary(df.pred.lr, lev = levels(df.pred.lr$obs))
twoClassSummary(df.pred.nb, lev = levels(df.pred.lr$obs))
twoClassSummary(df.pred.ct, lev = levels(df.pred.lr$obs))
twoClassSummary(df.pred.svm, lev = levels(df.pred.lr$obs))

mnLogLoss(df.pred.lr, lev = levels(df.pred.lr$obs))
mnLogLoss(df.pred.nb, lev = levels(df.pred.lr$obs))
mnLogLoss(df.pred.ct, lev = levels(df.pred.lr$obs))
mnLogLoss(df.pred.svm, lev = levels(df.pred.lr$obs))

```

### 2.6 ROC curve and probability threshold tuning

```{r, echo = T, message = FALSE,warning=FALSE}


library(pROC)
ROC.lr <- roc(obs ~ Yes, data=df.pred.lr)
ROC.nb <- roc(obs ~ Yes, data=df.pred.nb)
ROC.ct <- roc(obs ~ Yes, data=df.pred.ct)
ROC.svm <- roc(obs ~ Yes, data=df.pred.svm)

plot(ROC.lr, print.thres="best")
plot(ROC.nb, print.thres="best", add=TRUE)
plot(ROC.ct, print.thres="best", add=TRUE)
plot(ROC.svm, print.thres="best", add=TRUE)

Doc.lr.prob.tr <- predict(Doc.lr, newdata=df.tr, type="response")
df.pred.lr.tr <- data.frame(obs=df.tr$visits,
                            Yes=Doc.lr.prob.tr)
ROC.lr.tr <- roc(obs ~ Yes, data=df.pred.lr.tr)
plot(ROC.lr.tr, print.thres="best")

Doc.lr.pred.opt <- ifelse(Doc.lr.prob>0.193,"Yes","No")
confusionMatrix(data=as.factor(Doc.lr.pred.opt), reference = df.te$visits)
```

# Course 8 Data splitting

## 1 Prepare the data, set aside the test set

```{r, echo = T, message = FALSE,warning=FALSE}
DocVis <- read.csv("DocVis.csv") ## adapt the path
DocVis$visits <- as.factor(DocVis$visits) ## make sure that visits is a factor

library(caret)
library(dplyr)
set.seed(346)
index.tr <- createDataPartition(y = DocVis$visits, p= 0.8, list = FALSE)
df.tr <- DocVis[index.tr,]
df.te <- DocVis[-index.tr,]

```

## 2 A 10-fold Cross-validation

### 2.1 First fold

```{r, echo = T, message = FALSE,warning=FALSE}
index.CV <- createFolds(y = df.tr$visits, k=10)
index.CV[[1]]

df.cv.tr <- df.tr[-index.CV[[1]],]
df.cv.val <- df.tr[index.CV[[1]],]

Doc.cv <- glm(visits~., data=df.cv.tr, family="binomial") %>% step()
Doc.cv.prob <- predict(Doc.cv, newdata=df.cv.val, type="response")
Doc.cv.pred <- ifelse(Doc.cv.prob>0.5,"Yes","No")
confusionMatrix(data=as.factor(Doc.cv.pred), reference = df.cv.val$visits)$overall[1]

```

### 2.2 Loop on the 10 folds

```{r, echo = T, message = FALSE,warning=FALSE}

acc.vec <- numeric(10)
for (i in 1:10){
  df.cv.tr <- df.tr[-index.CV[[i]],]
  df.cv.val <- df.tr[index.CV[[i]],]
  
  Doc.cv <- glm(visits~., data=df.cv.tr, family="binomial") %>% step(trace=0)
  Doc.cv.prob <- predict(Doc.cv, newdata=df.cv.val, type="response")
  Doc.cv.pred <- ifelse(Doc.cv.prob>0.5,"Yes","No")
  acc.vec[i] <- confusionMatrix(data=as.factor(Doc.cv.pred), reference = df.cv.val$visits)$overall[1]
}
acc.vec

mean(acc.vec)
sd(acc.vec)

```

### 2.3 With caret

```{r, echo = T, message = FALSE,warning=FALSE}

trctrl <- trainControl(method = "cv", number=10)

set.seed(346)
Doc.cv <- train(visits ~., data = df.tr, method = "glmStepAIC", family="binomial",
                trControl=trctrl, trace=0)
Doc.cv

Doc.pred <- predict(Doc.cv, newdata = df.te)
confusionMatrix(data=Doc.pred, reference = df.te$visits)

```


## 3 Bootstrap with 100 replicates

### 3.1 First sample

```{r, echo = T, message = FALSE,warning=FALSE}

set.seed(897)
index.boot <- createResample(y=df.tr$visits, times=100)
index.boot[[1]]

df.boot.tr <- df.tr[index.boot[[1]],]
dim(df.boot.tr)
df.boot.val <- df.tr[-index.boot[[1]],]
dim(df.boot.val)

Doc.boot <- glm(visits~., data=df.boot.tr, family="binomial") %>% step()

Doc.boot.prob.val <- predict(Doc.boot, newdata=df.boot.val, type="response")
Doc.boot.pred.val <- ifelse(Doc.boot.prob.val>0.5,"Yes","No")
oob.acc <- confusionMatrix(data=as.factor(Doc.boot.pred.val), reference = df.boot.val$visits)$overall[1]

Doc.boot.prob.tr <- predict(Doc.boot, newdata=df.boot.tr, type="response")
Doc.boot.pred.tr <- ifelse(Doc.boot.prob.tr>0.5,"Yes","No")
app.acc <- confusionMatrix(data=as.factor(Doc.boot.pred.tr), reference = df.boot.tr$visits)$overall[1]

oob.acc ## out-of-bag accuracy
app.acc ## apparent accuracy
0.368*app.acc + 0.632*oob.acc ## accuracy estimate

```

### 3.2 Loop on the 100 sample

```{r, echo = T, message = FALSE,warning=FALSE}

oob.acc.vec <- numeric(100)
app.acc.vec <- numeric(100)
acc.vec <- numeric(100)
for (i in 1:100){
  df.boot.tr <- df.tr[index.boot[[i]],]
  df.boot.val <- df.tr[-index.boot[[i]],]
  
  Doc.boot <- glm(visits~., data=df.boot.tr, family="binomial") %>% step(trace=0)
  Doc.boot.prob.val <- predict(Doc.boot, newdata=df.boot.val, type="response")
  Doc.boot.pred.val <- ifelse(Doc.boot.prob.val>0.5,"Yes","No")
  oob.acc.vec[i] <- confusionMatrix(data=as.factor(Doc.boot.pred.val), reference = df.boot.val$visits)$overall[1]
  
  Doc.boot.prob.tr <- predict(Doc.boot, newdata=df.boot.tr, type="response")
  Doc.boot.pred.tr <- ifelse(Doc.boot.prob.tr>0.5,"Yes","No")
  app.acc.vec[i] <- confusionMatrix(data=as.factor(Doc.boot.pred.tr), reference = df.boot.tr$visits)$overall[1]
  
  acc.vec[i] <- 0.368*app.acc.vec[i] + 0.632*oob.acc.vec[i]
}

acc.vec

mean(acc.vec)
sd(acc.vec)

```

### 3.3 With caret

```{r, echo = T, message = FALSE,warning=FALSE}

set.seed(346)
trctrl <- trainControl(method = "boot632", number=100)
Doc.boot <- train(visits ~., data = df.tr, method = "glmStepAIC", family="binomial",
                  trControl=trctrl, trace = 0)
Doc.boot


```

## 4 Balancing data

```{r, echo = T, message = FALSE,warning=FALSE}

## Statistics on the training set
table(df.tr$visits)

Doc.lr <- glm(visits~., data=df.tr, family="binomial") %>% step(trace=0)
Doc.lr.prob <- predict(Doc.lr, newdata=df.te, type="response")
Doc.lr.pred <- ifelse(Doc.lr.prob>0.5,"Yes","No")
confusionMatrix(data=as.factor(Doc.lr.pred), reference = df.te$visits)

```

### 4.1 Sub-sampling

```{r, echo = T, message = FALSE,warning=FALSE}

n.yes <- min(table(df.tr$visits)) ## 840

df.tr.no <- filter(df.tr, visits=="No") ## the "No" cases
df.tr.yes <- filter(df.tr, visits=="Yes") ## The "Yes" cases

index.no <- sample(size=n.yes, x=1:nrow(df.tr.no), replace=FALSE) ## sub-sample 840 instances from the "No"

df.tr.subs <- data.frame(rbind(df.tr.yes, df.tr.no[index.no,])) ## Bind all the "Yes" and the sub-sampled "No"
table(df.tr.subs$visits) ## The cases are balanced

Doc.lr.subs <- glm(visits~., data=df.tr.subs, family="binomial") %>% step(trace=0)
Doc.lr.subs.prob <- predict(Doc.lr.subs, newdata=df.te, type="response")
Doc.lr.subs.pred <- ifelse(Doc.lr.subs.prob>0.5,"Yes","No")
confusionMatrix(data=as.factor(Doc.lr.subs.pred), reference = df.te$visits)

```

### 4.2 Resampling

```{r, echo = T, message = FALSE,warning=FALSE}
n.no <- max(table(df.tr$visits)) ## 3313

df.tr.no <- filter(df.tr, visits=="No")
df.tr.yes <- filter(df.tr, visits=="Yes")

index.yes <- sample(size=n.no, x=1:nrow(df.tr.yes), replace=TRUE)
df.tr.res <- data.frame(rbind(df.tr.no, df.tr.yes[index.yes,]))
table(df.tr.res$visits)

Doc.lr.res <- glm(visits~., data=df.tr.res, family="binomial") %>% step(trace=0)
Doc.lr.res.prob <- predict(Doc.lr.res, newdata=df.te, type="response")
Doc.lr.res.pred <- ifelse(Doc.lr.res.prob>0.5,"Yes","No")
confusionMatrix(data=as.factor(Doc.lr.res.pred), reference = df.te$visits)

```


# Course 9 

```{r, echo = T, message = FALSE,warning=FALSE}


```


