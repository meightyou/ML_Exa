# ------------ MACHINE LEARNING CODE ------------------

```{r, echo = FALSE, message = FALSE, warning = FALSE}
source(here::here("scripts/setup.R"))
```

# Data Loading

```{r, echo = FALSE, message = FALSE}
DocVis <- read_csv("DocVis.csv")

german <- read_csv("german.csv")

nursing_data <- read_csv("nursing_data.csv")

real_estate <- read_csv("real_estate_data.csv")

Wine <- read_csv("Wine.csv")

imports.85 <- read.csv("imports-85.data", 
                       header=FALSE, na.strings="?")

```

# Course 0

## EDA

### Data Rename

```{r, echo = FALSE, message = FALSE}
names(imports.85) <- c("symboling", "normalized_losses", "make", "fuel_type", "aspiration",
                       "num_doors", "body_style", "drive_wheels", "engine_location", "wheel_base",
                       "length", "width", "height","curb_weight", "engine_type", "num_cylinders",
                       "engine_size", "fuel_system", "bore", "stroke", "compression_ratio", "horsepower",
                       "peak_rpm", "city_mpg", "highway_mpg", "price")
```

### Data structure and summary

```{r, echo = FALSE, message = FALSE}

str(imports.85)
head(imports.85)
summary(imports.85)

# Turning character columns into factors through a loop
for (i in 1:ncol(imports.85)){
  if (class(imports.85[,i])=="character"){
    imports.85[,i] <- factor(imports.85[,i])
  }
}
# same operation can also be done through the `lapply` function
# imports.85[sapply(imports.85, is.character)] <-
#   lapply(imports.85[sapply(imports.85, is.character)],
#          as.factor)

str(imports.85)
summary(imports.85)

dfSummary(imports.85, style="grid")

view(dfSummary(imports.85, style="grid",plain.ascii = FALSE, tmp.img.dir = "/tmp"))

## first look at the current levels
levels(imports.85$fuel_system)
## Now rename the levels merging 4bbl[3], mfi[5], spfi[8] to "other"
levels(imports.85$fuel_system)[c(3,5,8)] <- "other"

## see the result
summary(imports.85)

```

### Graph for each variable

```{r, echo = FALSE, message = FALSE}


## Individually
p1 <- ggplot(imports.85, aes(x="price", y=price)) + 
  geom_boxplot()+xlab("")
p1
p2 <- ggplot(imports.85, aes(x="price", y=price)) + 
  geom_violin()+xlab("")
p2
p3 <- ggplot(imports.85, aes(x=price)) + 
  geom_histogram(color="black", fill="white")
p3
## The 3 plots on the same page
grid.arrange(p1, p2, p3, nrow=2, layout_matrix=matrix(c(1,2,3,3), byrow=TRUE, nc=2))

## pie chart with the basic plot function
## (ggplot2 is not good at producing pie charts)
pie(table(imports.85$fuel_system))

## bar plots
imports.85 %>% count(fuel_system) %>%
  ggplot(aes(y=n, x=fuel_system)) + 
  geom_bar(stat="identity")+coord_flip()

imports.85 %>% count(fuel_system) %>%
  ggplot(aes(fill=fuel_system, y=n, x="Fuel System")) + 
  geom_bar(position="stack", stat="identity", color="black")+xlab("")


## pie chart with the basic plot function
## (ggplot2 is not good at producing pie charts)
pie(table(imports.85$fuel_system))

## bar plots
imports.85 %>% count(fuel_system) %>%
  ggplot(aes(y=n, x=fuel_system)) + 
  geom_bar(stat="identity")+coord_flip()

imports.85 %>% count(fuel_system) %>%
  ggplot(aes(fill=fuel_system, y=n, x="Fuel System")) + 
  geom_bar(position="stack", stat="identity", color="black")+xlab("")
```

### Several graphs

```{r, echo = FALSE, message = FALSE}

## For the numerical variables
imports.85 %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()

## For the categorical variables
imports.85 %>%
  keep(is.factor) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_bar(stat="count")+coord_flip()

imports.85 %>%
  keep(is.numeric) %>% 
  gather()
```

### Scatterplot matrix for several variables

```{r, echo = FALSE, message = FALSE}
library(GGally)

imports.85 %>% select(length, height, width, price) %>% 
 ggpairs()

imports.85 %>% select(length, height, width, price, fuel_system) %>% 
  ggpairs(columns=1:4, aes(colour=fuel_system))

imports.85 %>% select(fuel_system, fuel_type, aspiration) %>% 
  ggpairs()

imports.85 %>% select(body_style, num_doors, width, price) %>% 
  ggpairs()

```

## Recoding and feature engineering

### Building a new feature

```{r, echo = FALSE, message = FALSE}

imports.85 %>% mutate(volume=height*width*length) %>% 
  select(price, height, width, length, volume) %>% ggpairs
```

### Recoding a variable

```{r, echo = FALSE, message = FALSE}

imports.85 %>% mutate(num_cyl_num=case_when(
  num_cylinders=="two"~2, 
  num_cylinders=="three"~3, 
  num_cylinders=="four"~4, 
  num_cylinders=="five"~5, 
  num_cylinders=="six"~6, 
  num_cylinders=="eight"~8, 
  num_cylinders=="twelve"~12)) %>% 
  select(price, num_cyl_num, num_cylinders) %>% ggpairs

library(fastDummies)
imports.85 %>% select(fuel_system) %>% dummy_cols(remove_first_dummy = TRUE)

```



# COURSE 1 : K-Nearest Neighbours

## 1 K-NN: a gentle introduction

```{r, echo = FALSE, message = FALSE}
library(dplyr)
library(caret)
iris
iris <- iris

mod.knn <- knn3(data = iris, Species~., k = 2) ## build the knn model
predict(mod.knn, newdata = iris, type = "class")

set.seed(123) ## for replication purpose

index.tr <- sample(1:nrow(iris), replace=FALSE, size=0.75*nrow(iris))
index.tr ## the index of the rows of iris that will be in the training set

iris.tr <- iris[index.tr,] ## the training set
iris.te <- iris[-index.tr,] ## the test set, the minus sign is telling take ALL BUT the training set

mod.knn <- knn3(data=iris.tr, Species~., k=2)
iris.te.pred <- predict(mod.knn, newdata = iris.te, type="class")

table(Pred=iris.te.pred, Observed=iris.te[,5])

mod.knn <- knn3(data=iris.tr, Species~., k=3)
iris.te.pred <- predict(mod.knn, newdata = iris.te, type="class")

table(Pred=iris.te.pred, Observed=iris.te[,5])

class(mod.knn) # class = knn

```

## 2 K-NN REGRESSION

```{r, echo = FALSE, message = FALSE}
imports.85 <- read.csv("imports-85.data", 
                       header=FALSE, na.strings="?")# replaces the ? with NA

names(imports.85) <- c("symboling", "normalized_losses", "make", "fuel_type", "aspiration",
                       "num_doors", "body_style", "drive_wheels", "engine_location", "wheel_base",
                       "length", "width", "height","curb_weight", "engine_type", "num_cylinders",
                       "engine_size", "fuel_system", "bore", "stroke", "compression_ratio", "horsepower",
                       "peak_rpm", "city_mpg", "highway_mpg", "price")#renaming the columns because not specified in the doc




tmp <- imports.85 %>% select(where(is.numeric))#only select columns with numerical data
tmp <- filter(tmp, complete.cases(tmp))#and then only select rows where there is no NAs

tmp# tmp means temporary

set.seed(123)
index.tr <- sample(1:nrow(tmp), size=0.75*nrow(tmp), replace = FALSE)
tmp.tr <- tmp[index.tr,]
tmp.te <- tmp[-index.tr,]
mod.knn <- knnreg(data=tmp.tr, price~., k=3)# 3 nearest neighbours
tmp.pred <- predict(mod.knn, newdata=tmp.te)

tmp.tr#just to view
tmp.te#just to view

tmp.te %>% mutate(pred=tmp.pred) %>%
  ggplot(aes(x=price, y=pred)) + 
  geom_point() + geom_abline(slope=1, intercept = 0)

```

## 3 K-NN: mixture of feature types

```{r, echo = FALSE, message = FALSE}

## tmp is imports.85 without incomplete cases and without price
imp.comp <- imports.85 %>% filter(complete.cases(imports.85))#select only rows with no missing data
y <- imp.comp$price#y variable
imp.comp <- imp.comp %>% select(!price)#x variable = all but y variable

## The numerical variables in tmp are standardized
imp.num <- imp.comp %>% select(where(is.numeric)) %>% scale() %>% as.data.frame()

## the dummy variables of the numerical features of tmp are created
library(fastDummies)
imp.dumm <- imp.comp %>% select(!where(is.numeric)) %>% 
  dummy_cols(remove_first_dummy = FALSE, remove_selected_columns = TRUE)

## These dummy variables are added to tmp
imp.dat <- data.frame(imp.num, imp.dumm)

## The names of the two problematic brands are changed
names(imp.dat)[c(16,25)] <- c("make_alfa_romeo","make_mercredes_benz")

## The raw price is added to tmp
imp.dat$price <- y

set.seed(123)
index.tr <- sample(1:nrow(imp.dat), size=0.75*nrow(imp.dat), replace = FALSE)
imp.tr <- imp.dat[index.tr,]
imp.te <- imp.dat[-index.tr,]
mod.knn <- knnreg(data=imp.tr, price~., k=3)
imp.pred <- predict(mod.knn, newdata=imp.te)
imp.te %>% mutate(pred=imp.pred) %>%
  ggplot(aes(x=price, y=pred)) + 
  geom_point() + geom_abline(slope=1, intercept = 0)

```

# Course 2 Models: Naive Bayes

## 1 Conditional density plots

```{r, echo = FALSE, message = FALSE}

library(naivebayes)
iris.nb <- naive_bayes(Species ~ .,
                       data = iris, usekernel=TRUE)
par(mfrow=c(2,2))
plot(iris.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8),
     prob="conditional")
par(mfrow=c(1,1))

iris.nb

iris.nb <- naive_bayes(Species ~ .,
                       data = iris, usekernel=FALSE) 
par(mfrow=c(2,2))
plot(iris.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8), prob="conditional")
par(mfrow=c(1,1))

iris.nb

```

## 2 Predictions

```{r, echo = FALSE, message = FALSE}

predict(iris.nb, newdata = iris[,-5])
predict(iris.nb, newdata = iris[,-5], type="prob")

```

## 3 Priors

```{r, echo = FALSE, message = FALSE}

iris.nb <- naive_bayes(Species ~ .,
                       data = iris, usekernel=FALSE, prior = c(0.1,0.9,0.1)) 
par(mfrow=c(2,2))
plot(iris.nb, arg.num = list(col = 1:3,
                             legend.position = "topright",
                             legend.cex = 0.8), prob="conditional")
par(mfrow=c(1,1))

predict(iris.nb, newdata = iris[,-5])

```

## 4 Categorical features

```{r, echo = FALSE, message = FALSE}

DocVis <- read.csv("DocVis.csv") # adapt the path to the file; use the Import Dataset tool
DocVis.nb <- naive_bayes(visits~., data=DocVis, usekernel = TRUE)
plot(DocVis.nb, prob = "conditional")

table(pred=predict(DocVis.nb), obs=DocVis$visits)

```


# Course 3 Models: Linear and logistic regressions

## 1 Linear regression for real estate transaction price predictions

### 1.1 The data

```{r, echo = FALSE, message = FALSE}

real_estate <- read_csv("real_estate_data.csv")

```

### 1.2 EDA

```{r, echo = FALSE, message = FALSE}

real_estate_data <- read.csv("real_estate_data.csv") ## adapt the path to the data
# if you encountered any error with the encoding of the data (`Error in gregexpr...`), just re-run the code again
str(real_estate_data)
library(summarytools)
dfSummary(real_estate_data)

library(dplyr)
library(ggplot2)
real_estate_data %>% ggplot(aes(x=Month, y=Price, fill=as.factor(Year))) + 
  geom_boxplot()+ facet_wrap(~as.factor(Year))

library(GGally)
real_estate_data %>% 
  select(Price, HouseAge, Dist, Lat, Long, TransDate) %>% 
  ggpairs()

```

### 1.3 Linear regression

```{r, echo = FALSE, message = FALSE}

set.seed(234)
index <- sample(x=c(1,2), size=nrow(real_estate_data), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set
dat.tr <- real_estate_data[index==1,]
dat.te <- real_estate_data[index==2,]

mod.lm <- lm(Price~TransDate+
               HouseAge+
               Dist+
               NumStores+
               Lat+
               Long, data=dat.tr)
summary(mod.lm)

```

### 1.4 Variable selection and interpretation

```{r, echo = FALSE, message = FALSE}

step(mod.lm) # see the result
mod.lm.sel <- step(mod.lm) # store the final model into mod.lm.sel
summary(mod.lm.sel)

```

### 1.5 Predictions

```{r, echo = FALSE, message = FALSE}

mod.lm.pred <- predict(mod.lm.sel, newdata=dat.te)
plot(dat.te$Price ~ mod.lm.pred, xlab="Prediction", ylab="Observed prices")
abline(0,1) # line showing the obs -- pred agreement

#Answer
mod.lm.pred[c(1,2)]
```

## 2 Logistic regression with visit data

### 2.1 The data

```{r, echo = FALSE, message = FALSE}

DocVis <- read.csv("DocVis.csv") ## adapt the path

```

### 2.2 The model

```{r, echo = FALSE, message = FALSE}

set.seed(234)
index <- sample(x=c(1,2), size=nrow(DocVis), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set
dat.tr <- DocVis[index==1,]
dat.te <- DocVis[index==2,]

vis.logr <- glm(visits~., data=dat.tr, family="binomial")
summary(vis.logr)


```

### 2.3 Variable selection and interpretation of the coefficients

```{r, echo = FALSE, message = FALSE}

vis.logr.sel <- step(vis.logr)
summary(vis.logr.sel)

```

### 2.4 Predictions

```{r, echo = FALSE, message = FALSE}

prob.te <- predict(vis.logr.sel, newdata = dat.te, type="response")
pred.te <- ifelse(prob.te >= 0.5, 1, 0)
table(Pred=pred.te, Obs=dat.te$visits)

boxplot(prob.te~dat.te$visits)

#Answer
prob.te[c(1,2)]


```
# Course 4 

## 1 Classification tree

### 1.1 Prepare the data
```{r, echo = FALSE, message = FALSE}
library(ISLR)
library(dplyr)
MyCarseats <- Carseats %>% mutate(SaleHigh=ifelse(Sales > 7.5, "Yes", "No"))
MyCarseats <- MyCarseats %>% select(-Sales)

set.seed(123) # for reproducibility 
index.tr <- sample(x=1:nrow(MyCarseats), size=2/3*nrow(MyCarseats), replace=FALSE)
df.tr <- MyCarseats[index.tr,]
df.te <- MyCarseats[-index.tr,]

```

### 1.2 Classification tree fit and plot

```{r, echo = FALSE, message = FALSE}
library(rpart)
library(rpart.plot)
set.seed(1234)
carseats.tree <- rpart(SaleHigh ~ ., data=df.tr)
rpart.plot(carseats.tree)

```

### 1.3 Pruning the tree

```{r, echo = FALSE, message = FALSE}
plotcp(carseats.tree)
carseats.tree.prune <- prune(carseats.tree, cp=0.025)
rpart.plot(carseats.tree.prune)
library(adabag)
set.seed(123455)
rpart.plot(autoprune(SaleHigh ~ ., data=df.tr))

```

### 1.4 Predictions

```{r, echo = FALSE, message = FALSE}
#answer 
MyCarseats[1,]

pred <- predict(carseats.tree.prune, newdata=df.te, type="class")
table(Pred=pred, Obs=df.te$SaleHigh)

predict(carseats.tree.prune, newdata=df.te, type="prob")

```

## 2 Regression tree

```{r, echo = FALSE, message = FALSE}


set.seed(123)
carseats.reg <- rpart(Sales ~ ., data=Carseats)
rpart.plot(carseats.reg)

carseat.reg.pred <- predict(carseats.reg)
plot(carseat.reg.pred ~ Carseats$Sales,
     xlab="Sales", ylab="Predictions")
abline(0,1, col="red")

```


# Course 5 Models: Neural Network

## 1 Neural Network (NN)

### 1.1 Build simple NNs with nnet

```{r, echo = FALSE, message = FALSE}

library(nnet)

nn1 <- nnet(Species~Sepal.Length+Petal.Width, data=iris, size=2)
summary(nn1)
nn2 <- nnet(Species~Sepal.Length+Petal.Width, data=iris, size=c(5,3))
nn3 <- nnet(Species~Sepal.Length+Petal.Width, data=iris, size=2, decay=3)

pred1 <- predict(nn1, type="class")
pred2 <- predict(nn2, type="class")
pred3 <- predict(nn3, type="class")

tab1 <- table(Obs=iris$Species, Pred=pred1)
tab2 <- table(Obs=iris$Species, Pred=pred2)
tab3 <- table(Obs=iris$Species, Pred=pred3)

(acc1 <- sum(diag(tab1))/sum(tab1))
(acc2 <- sum(diag(tab2))/sum(tab2))
(acc3 <- sum(diag(tab3))/sum(tab3))

```

### 1.2 nnet+caret: Simple hyperparameter tuning

```{r, echo = FALSE, message = FALSE}

library(caret)
# this can take a few seconds to run on your computer
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           #`cv` stands for `cross-validation` which we will 
                           #see during one of the upcoming courses
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1),
                         decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(Species ~ Sepal.Length+Petal.Width, 
                 data = iris,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

plot(nnetFit)

```

### 1.2.1 Visualizing NNs

```{r, echo = FALSE, message = FALSE}

library(neuralnet)
Species.hot <- class.ind(iris$Species) 
iris.hot <- cbind(iris[,1:4], Species.hot)
head(iris.hot)

f <- as.formula(paste("setosa+versicolor+virginica~", paste(names(iris.hot)[1:4], collapse = " + ")))
f
nn4 <- neuralnet(f, data=iris.hot, hidden=2)
nn5 <- neuralnet(f, data=iris.hot, hidden=c(2,3))
plot(nn4, rep="best")
plot(nn5, rep="best")

```

# Course 6 

```{r, echo = FALSE, message = FALSE}


```

# Course 7 

```{r, echo = FALSE, message = FALSE}


```

# Course 8 

```{r, echo = FALSE, message = FALSE}


```

# Course 9 

```{r, echo = FALSE, message = FALSE}


```


